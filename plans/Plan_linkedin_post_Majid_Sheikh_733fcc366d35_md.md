# Plan: linkedin_post_Majid_Sheikh_733fcc366d35.md

**Category:** feature
**Source:** inbox
**Sensitive:** Yes
**Generated:** 2026-02-23T22:12:52.429702+00:00
**Cycle:** Silver

---

## Summary
Task from inbox: linkedin_post_Majid_Sheikh_733fcc366d35.md. Review and complete as appropriate.

## Checklist

- [ ] Read and confirm task content understood
- [ ] Categorise as: feature
- [ ] Sensitivity check: Sensitive (Approved / Rejected)
- [ ] Define requirements from task
- [ ] Outline implementation steps
- [ ] List affected files/components
- [ ] Route to human-approval-workflow (sensitive)
- [ ] Update dashboard via dashboard-updater
- [ ] Move task file to done/

## Key Information Extracted
- # LinkedIn Notification — Post
- - **From:** Majid Sheikh
- - **Category:** post
- - **Notification time:** 12h
- - **Captured:** 2026-02-23T22:06:57.576388+00:00

## Original Content
```
# LinkedIn Notification — Post

- **From:** Majid Sheikh
- **Category:** post
- **Notification time:** 12h
- **Captured:** 2026-02-23T22:06:57.576388+00:00
- **Matched keywords:** posted

## Content

Unread notification.

Majid Sheikh posted: Most RAG systems fail in production. Not because of the model. Because of bad implementation. The common mistakes I see: • dumping raw documents without structure • no metadata or filtering • poor chunking strategy • no retrieval evaluation • no logging or monitoring Result? The AI gives inconsistent or incorrect answers. RAG works extremely well — but only if built properly. The model is just one piece. Retrieval quality determines everything.

12h

```

## Agent Notes
<!-- Routed to human-approval-workflow — sensitive task requires human review -->
